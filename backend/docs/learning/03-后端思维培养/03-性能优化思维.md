# 性能优化思维

> 性能优化是后端开发的重要能力，直接影响用户体验和系统成本。

## 性能优化的层次

```
┌─────────────────────────────────┐
│       应用层优化                 │
│  算法优化、缓存策略、异步处理    │
├─────────────────────────────────┤
│       数据库层优化               │
│  索引优化、查询优化、连接池      │
├─────────────────────────────────┤
│       网络层优化                 │
│  减少请求、压缩传输、CDN         │
└─────────────────────────────────┘
```

---

## 1. 应用层优化

### 缓存策略

```python
# 项目中的 Redis 缓存使用

# 1. 字典数据缓存
class RedisInitKeyConfig:
    SYS_DICT = "sys_dict"  # 字典缓存
    SYS_CONFIG = "sys_config"  # 配置缓存

# 2. 查询时优先从缓存获取
async def get_dict_data(dict_type: str):
    # 先查 Redis
    cached_data = await redis.get(f"sys_dict:{dict_type}")
    if cached_data:
        return json.loads(cached_data)

    # 缓存未命中，查数据库
    data = await db.execute(select(SysDictData).where(...))

    # 写入缓存
    await redis.set(f"sys_dict:{dict_type}", json.dumps(data), ex=3600)
    return data

# 3. 更新时清除缓存
async def update_dict_data(data: DictDataModel):
    await db.execute(update(SysDictData).values(...))
    # 清除缓存
    await redis.delete(f"sys_dict:{data.dict_type}")
```

**缓存设计要点**：

| 要点 | 说明 |
|------|------|
| 缓存什么 | 读多写少的数据（字典、配置） |
| 缓存多久 | 根据数据更新频率设置过期时间 |
| 如何更新 | 主动删除缓存，下次查询时重建 |
| 缓存穿透 | 不存在的数据也缓存空值 |
| 缓存雪崩 | 过期时间加随机值 |

### 异步处理

```python
# 同步方式（阻塞）
async def create_order(order: OrderModel):
    # 1. 创建订单
    order_id = await create_order_in_db(order)

    # 2. 扣减库存（慢）
    await update_inventory(order.product_id, order.quantity)

    # 3. 发送通知（慢）
    await send_notification(order.user_id, "订单创建成功")

    return order_id

# 异步方式（非阻塞）
async def create_order(order: OrderModel):
    # 1. 创建订单
    order_id = await create_order_in_db(order)

    # 2. 异步扣减库存
    asyncio.create_task(update_inventory(order.product_id, order.quantity))

    # 3. 异步发送通知
    asyncio.create_task(send_notification(order.user_id, "订单创建成功"))

    return order_id  # 立即返回，不等后续操作
```

### 批量操作

```python
# ❌ N+1 查询问题
async def get_users_with_depts():
    users = await db.execute(select(SysUser))  # 1 次查询
    result = []
    for user in users:
        dept = await db.execute(
            select(SysDept).where(SysDept.dept_id == user.dept_id)
        )  # N 次查询
        result.append({"user": user, "dept": dept})
    # 总共 1 + N 次查询

# ✅ 使用 JOIN 一次查询
async def get_users_with_depts():
    result = await db.execute(
        select(SysUser, SysDept)
        .join(SysDept, SysUser.dept_id == SysDept.dept_id)
    )  # 只 1 次查询
    return result
```

### 连接池配置

```python
# config/database.py
async_engine = create_async_engine(
    DATABASE_URL,
    # 连接池配置
    pool_size=10,           # 连接池大小
    max_overflow=20,        # 最大溢出连接数
    pool_timeout=30,        # 获取连接超时时间
    pool_recycle=3600,      # 连接回收时间（1小时）
    pool_pre_ping=True,     # 连接前检测可用性

    # 性能优化
    echo=False,             # 不打印 SQL（生产环境）
    echo_pool=False,        # 不打印连接池日志
)
```

---

## 2. 数据库层优化

### 索引优化

```python
# 分析慢查询
# 方式1：使用 EXPLAIN
EXPLAIN SELECT * FROM sys_user WHERE user_name = 'admin';

# 方式2：开启慢查询日志
# MySQL 配置
slow_query_log = 1
long_query_time = 1  # 超过1秒的查询记录

# 索引设计原则
class SysUser(Base):
    __table_args__ = (
        # WHERE 条件的字段
        Index('idx_user_status', 'status', 'del_flag'),

        # JOIN 的字段
        Index('idx_user_dept', 'dept_id'),

        # ORDER BY 的字段
        Index('idx_user_time', 'create_time'),

        # 组合索引（注意顺序）
        Index('idx_user_search', 'dept_id', 'status', 'del_flag'),
        # 最左前缀原则：
        # ✅ WHERE dept_id = 1
        # ✅ WHERE dept_id = 1 AND status = '0'
        # ❌ WHERE status = '0'  # 不会使用索引
    )
```

### 查询优化

```python
# ✅ 只查询需要的字段
stmt = select(SysUser.user_id, SysUser.user_name, SysUser.email)

# ❌ 不要使用 SELECT *
stmt = select(SysUser)

# ✅ 使用 LIMIT 分页
stmt = select(SysUser).offset(0).limit(10)

# ✅ 使用 EXISTS 替代 IN
# 慢
stmt = select(SysUser).where(
    SysUser.dept_id.in_(select(SysDept.dept_id).where(...))
)
# 快
stmt = select(SysUser).where(
    exists().where(SysUser.dept_id == SysDept.dept_id)
)

# ✅ 批量插入
await db.execute(
    insert(SysUser),
    [
        {"user_name": "user1", "email": "user1@example.com"},
        {"user_name": "user2", "email": "user2@example.com"},
    ]
)
```

### 事务优化

```python
# ❌ 事务时间过长
async def bad_transaction():
    async with db.begin():
        # 数据库操作
        await db.execute(...)

        # 调用外部API（慢！）
        await external_api_call()

        # 发送邮件（慢！）
        await send_email()

# ✅ 事务只包含数据库操作
async def good_transaction():
    # 数据库事务
    async with db.begin():
        await db.execute(...)
        await db.execute(...)

    # 事务外执行其他操作
    await external_api_call()
    await send_email()
```

---

## 3. 网络层优化

### 减少请求次数

```python
# ❌ 多次请求
async def get_user_data(user_id: int):
    # 请求1：获取基本信息
    user = await get_user_info(user_id)

    # 请求2：获取角色
    roles = await get_user_roles(user_id)

    # 请求3：获取权限
    permissions = await get_user_permissions(user_id)

    return {**user, "roles": roles, "permissions": permissions}

# ✅ 合并请求
async def get_user_data(user_id: int):
    # 一次请求返回所有数据
    return await get_user_complete_info(user_id)
```

### 数据压缩

```python
# middlewares/gzip_middleware.py
from fastapi.middleware.gzip import GZipMiddleware

app.add_middleware(
    GZipMiddleware,
    minimum_size=1000  # 大于1KB的响应才压缩
)

# 响应头自动添加
# Content-Encoding: gzip
```

### 分页优化

```python
# ❌ 深分页性能问题
# OFFSET 越大，性能越差
SELECT * FROM sys_user ORDER BY user_id LIMIT 10 OFFSET 100000

# ✅ 使用游标分页
# 记录最后一条的 ID，下次查询从此开始
SELECT * FROM sys_user WHERE user_id > last_id ORDER BY user_id LIMIT 10
```

---

## 4. 代码优化

### 算法优化

```python
# ❌ O(n²) - 嵌套循环
def find_duplicates_slow(users):
    duplicates = []
    for i, user1 in enumerate(users):
        for user2 in users[i+1:]:
            if user1.email == user2.email:
                duplicates.append(user1)
    return duplicates

# ✅ O(n) - 使用集合
def find_duplicates_fast(users):
    seen = set()
    duplicates = []
    for user in users:
        if user.email in seen:
            duplicates.append(user)
        seen.add(user.email)
    return duplicates
```

### 字符串处理

```python
# ❌ 拼接字符串效率低
result = ""
for item in items:
    result += str(item) + ","

# ✅ 使用 join
result = ",".join(str(item) for item in items)

# ❌ 多次调用 replace
text = text.replace("a", "b").replace("c", "d").replace("e", "f")

# ✅ 使用正则表达式
import re
text = re.sub(r"[ace]", lambda m: {"a": "b", "c": "d", "e": "f"}[m.group()], text)
```

---

## 5. 监控与分析

### 性能监控

```python
# 添加性能日志
import time
from functools import wraps

def log_performance(func):
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start = time.time()
        result = await func(*args, **kwargs)
        end = time.time()

        # 记录慢查询
        if end - start > 1:  # 超过1秒
            logger.warning(f"{func.__name__} took {end - start:.2f}s")

        return result
    return wrapper

@log_performance
async def get_user_list():
    # 业务逻辑
    pass
```

### 性能分析工具

```bash
# 1. 数据库慢查询日志
# MySQL
tail -f /var/log/mysql/slow-query.log

# 2. 应用性能分析
# 安装
pip install py-spy

# 运行时分析
py-spy record --output profile.svg --pid <PID>

# 3. 内存分析
pip install memory_profiler

python -m memory_profiler app.py
```

---

## 6. 优化检查清单

### 缓存
- [ ] 读多写少的数据已缓存
- [ ] 缓存有合理的过期时间
- [ ] 更新数据时清除缓存
- [ ] 防止缓存穿透和雪崩

### 数据库
- [ ] WHERE 条件字段有索引
- [ ] 避免使用 SELECT *
- [ ] 使用批量操作
- [ ] 避免在事务中执行慢操作

### 代码
- [ ] 避免不必要的嵌套循环
- [ ] 使用高效的数据结构
- [ ] 避免重复计算
- [ ] 异步处理耗时操作

### 网络
- [ ] 合并请求减少次数
- [ ] 启用压缩
- [ ] 使用分页
- [ ] 优化响应大小

---

## 性能优化原则

1. **测量优先**
```
先测量，再优化
找出真正的瓶颈

❌ 凭感觉优化
✅ 用数据说话
```

2. **过早优化是万恶之源**
```
先保证正确性，再考虑性能

❌ 还没写完就想着优化
✅ 功能正常后再优化
```

3. **权衡取舍**
```
性能 vs 可读性
性能 vs 开发成本
性能 vs 维护成本

选择合适的平衡点
```

**下一步**: 学习 [安全考虑思维](./04-安全考虑思维.md)
